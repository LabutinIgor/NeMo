{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS Inference\n",
    "\n",
    "This notebook can be used to generate audio samples using either NeMo's pretrained models or after training NeMo TTS models. This script currently uses a two step inference procedure. First, a model is used to generate a mel spectrogram from text. Second, a model is used to generate audio from a mel spectrogram.\n",
    "\n",
    "Currently supported models are:\n",
    "Mel Spectrogram Generators:\n",
    "- Tacotron 2\n",
    "- Glow-TTS\n",
    "\n",
    "Audio Generators\n",
    "- Grifflin-Lim\n",
    "- WaveGlow\n",
    "- SqueezeWave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2020 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    "> \n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\\nInstructions for setting up Colab are as follows:\\n1. Open a new Python 3 notebook.\\n2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\\n3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\\n4. Run this cell to set up dependencies.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# # If you're using Google Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode\n",
    "# !pip install git+git://github.com/nvidia/NeMo.git@main#egg=nemo_toolkit[tts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose one of the following spectrogram generators:\n",
      "['tacotron2', 'glow_tts', 'talknet']\n",
      "talknet\n",
      "Choose one of the following audio generators:\n",
      "['griffin_lim', 'waveglow', 'squeezewave']\n",
      "waveglow\n"
     ]
    }
   ],
   "source": [
    "supported_spec_gen = [\"tacotron2\", \"glow_tts\", \"talknet\"]\n",
    "supported_audio_gen = [\"griffin_lim\", \"waveglow\", \"squeezewave\"]\n",
    "\n",
    "print(\"Choose one of the following spectrogram generators:\")\n",
    "print([model for model in supported_spec_gen])\n",
    "spectrogram_generator = input()\n",
    "print(\"Choose one of the following audio generators:\")\n",
    "print([model for model in supported_audio_gen])\n",
    "audio_generator = input()\n",
    "\n",
    "assert spectrogram_generator in supported_spec_gen\n",
    "assert audio_generator in supported_audio_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model checkpoints\n",
    "\n",
    "Note: For best quality with Glow TTS, please update the glow tts yaml file with the path to cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-10-02 10:13:42 modelPT:101] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      cls: nemo.collections.asr.data.audio_to_text.AudioToCharWithDursDataset\n",
      "      params:\n",
      "        manifest_filepath: /home/iglabutin/work/data/sets/ljspeech/LJSpeech-1.1/ljspeech_train.json\n",
      "        max_duration: null\n",
      "        min_duration: 0.1\n",
      "        int_values: false\n",
      "        load_audio: true\n",
      "        normalize: false\n",
      "        sample_rate: 22050\n",
      "        trim: false\n",
      "        vocab_notation: phonemes\n",
      "        vocab_punct: true\n",
      "        vocab_spaces: true\n",
      "        vocab_stresses: false\n",
      "        durs_path: /home/iglabutin/work/data/sets/ljspeech/nemo/durs/phonemes/punct-T_stresses-F_spaces-T.pth\n",
      "        rep: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 64\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2020-10-02 10:13:42 modelPT:108] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      cls: nemo.collections.asr.data.audio_to_text.AudioToCharWithDursDataset\n",
      "      params:\n",
      "        manifest_filepath: /home/iglabutin/work/data/sets/ljspeech/LJSpeech-1.1/ljspeech_eval.json\n",
      "        max_duration: null\n",
      "        min_duration: 0.1\n",
      "        int_values: false\n",
      "        load_audio: true\n",
      "        normalize: false\n",
      "        sample_rate: 22050\n",
      "        trim: false\n",
      "        vocab_notation: phonemes\n",
      "        vocab_punct: true\n",
      "        vocab_spaces: true\n",
      "        vocab_stresses: false\n",
      "        durs_path: /home/iglabutin/work/data/sets/ljspeech/nemo/durs/phonemes/punct-T_stresses-F_spaces-T.pth\n",
      "        rep: true\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 64\n",
      "      num_workers: 8\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-02 10:13:42 features:241] PADDING: 1\n",
      "[NeMo I 2020-10-02 10:13:42 features:254] STFT using conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-10-02 10:13:43 modelPT:101] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      cls: nemo.collections.asr.data.audio_to_text.AudioToCharWithDursDataset\n",
      "      params:\n",
      "        manifest_filepath: /home/iglabutin/work/data/sets/ljspeech/LJSpeech-1.1/ljspeech_train.json\n",
      "        max_duration: null\n",
      "        min_duration: 0.1\n",
      "        int_values: false\n",
      "        load_audio: false\n",
      "        normalize: false\n",
      "        sample_rate: 22050\n",
      "        trim: false\n",
      "        vocab_notation: phonemes\n",
      "        vocab_punct: true\n",
      "        vocab_spaces: true\n",
      "        vocab_stresses: false\n",
      "        durs_path: /home/iglabutin/work/data/sets/ljspeech/nemo/durs/phonemes/punct-T_stresses-F_spaces-T.pth\n",
      "        rep: false\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 64\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2020-10-02 10:13:43 modelPT:108] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      cls: nemo.collections.asr.data.audio_to_text.AudioToCharWithDursDataset\n",
      "      params:\n",
      "        manifest_filepath: /home/iglabutin/work/data/sets/ljspeech/LJSpeech-1.1/ljspeech_eval.json\n",
      "        max_duration: null\n",
      "        min_duration: 0.1\n",
      "        int_values: false\n",
      "        load_audio: false\n",
      "        normalize: false\n",
      "        sample_rate: 22050\n",
      "        trim: false\n",
      "        vocab_notation: phonemes\n",
      "        vocab_punct: true\n",
      "        vocab_spaces: true\n",
      "        vocab_stresses: false\n",
      "        durs_path: /home/iglabutin/work/data/sets/ljspeech/nemo/durs/phonemes/punct-T_stresses-F_spaces-T.pth\n",
      "        rep: false\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 64\n",
      "      num_workers: 8\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-02 10:13:43 cloud:55] Found existing object /home/iglabutin/.cache/torch/NeMo/NeMo_1.0.0b1/WaveGlow-22050Hz/63a329dc3e8b44ec2e07cd4209eeab2a/WaveGlow-22050Hz.nemo.\n",
      "[NeMo I 2020-10-02 10:13:43 cloud:61] Re-using file from: /home/iglabutin/.cache/torch/NeMo/NeMo_1.0.0b1/WaveGlow-22050Hz/63a329dc3e8b44ec2e07cd4209eeab2a/WaveGlow-22050Hz.nemo\n",
      "[NeMo I 2020-10-02 10:13:43 common:395] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2020-10-02 10:13:48 features:241] PADDING: 16\n",
      "[NeMo I 2020-10-02 10:13:48 features:254] STFT using conv\n",
      "[NeMo I 2020-10-02 10:13:50 modelPT:237] Model WaveGlowModel was successfully restored from /home/iglabutin/.cache/torch/NeMo/NeMo_1.0.0b1/WaveGlow-22050Hz/63a329dc3e8b44ec2e07cd4209eeab2a/WaveGlow-22050Hz.nemo.\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf, open_dict\n",
    "import torch\n",
    "from ruamel.yaml import YAML\n",
    "from nemo.collections.asr.parts import parsers\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "NFFT = 1024\n",
    "NMEL = 80\n",
    "FMAX = None\n",
    "\n",
    "def load_spectrogram_model():\n",
    "    if spectrogram_generator == \"tacotron2\":\n",
    "        from nemo.collections.tts.models import Tacotron2Model as SpecModel\n",
    "        pretrained_model = \"Tacotron2-22050Hz\"\n",
    "    elif spectrogram_generator == \"glow_tts\":\n",
    "        from nemo.collections.tts.models import GlowTTSModel as SpecModel\n",
    "        pretrained_model = \"GlowTTS-22050Hz\"\n",
    "    elif spectrogram_generator == \"talknet\":\n",
    "        from nemo.collections.tts.models import TalkNetDursModel\n",
    "        from nemo.collections.tts.models import TalkNetSpectModel\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if spectrogram_generator == \"talknet\":\n",
    "        model = TalkNetSpectModel.load_from_checkpoint(checkpoint_path=\"./sn2.ckpt\")\n",
    "        model.load_durs_predictor(\"./dn2.ckpt\")\n",
    "    else:\n",
    "        model = SpecModel.from_pretrained(pretrained_model)\n",
    "    \n",
    "    with open_dict(model._cfg):\n",
    "        global SAMPLE_RATE\n",
    "        global NFFT\n",
    "        global NMEL\n",
    "        global FMAX\n",
    "        SAMPLE_RATE = model._cfg.sample_rate or SAMPLE_RATE\n",
    "        NFFT = model._cfg.n_fft or NFFT\n",
    "        NMEL = model._cfg.n_mels or NMEL\n",
    "        FMAX = model._cfg.fmax or FMAX\n",
    "    return model\n",
    "\n",
    "def load_vocoder_model():\n",
    "    if audio_generator == \"waveglow\":\n",
    "        from nemo.collections.tts.models import WaveGlowModel as VocoderModel\n",
    "        pretrained_model = \"WaveGlow-22050Hz\"\n",
    "    elif audio_generator == \"squeezewave\":\n",
    "        from nemo.collections.tts.models import SqueezeWaveModel as VocoderModel\n",
    "        pretrained_model = \"SqueezeWave-22050Hz\"\n",
    "    elif audio_generator == \"griffin_lim\":\n",
    "        from nemo.collections.tts.helpers.helpers import griffin_lim\n",
    "        import numpy as np\n",
    "        import librosa\n",
    "        class GL:\n",
    "            def __init__(self):\n",
    "                pass\n",
    "            def convert_spectrogram_to_audio(self, spec):\n",
    "                log_mel_spec = spec.squeeze().to('cpu').numpy().T\n",
    "                mel_spec = np.exp(log_mel_spec)\n",
    "                mel_pseudo_inverse = librosa.filters.mel(SAMPLE_RATE, NFFT, NMEL, fmax=FMAX)\n",
    "                return griffin_lim(np.dot(mel_spec, mel_pseudo_inverse).T ** 1.2)\n",
    "            def load_state_dict(self, *args, **kwargs):\n",
    "                pass\n",
    "            def cuda(self, *args, **kwargs):\n",
    "                return self\n",
    "        return GL()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    model = VocoderModel.from_pretrained(pretrained_model)\n",
    "    with open_dict(model._cfg):\n",
    "        global SAMPLE_RATE\n",
    "        global NFFT\n",
    "        global NMEL\n",
    "        global FMAX\n",
    "        if model._cfg.sample_rate is not None and SAMPLE_RATE is not None:\n",
    "            assert model._cfg.sample_rate == SAMPLE_RATE\n",
    "        if model._cfg.n_fft is not None and NFFT is not None:\n",
    "            assert _cfg.n_fft == NFFT\n",
    "        if model._cfg.n_mels is not None and NMEL is not None:\n",
    "            assert model._cfg.n_mels == NMEL\n",
    "        if model._cfg.fmax is not None and FMAX is not None:\n",
    "            assert model._cfg.fmax == FMAX\n",
    "    return model\n",
    "\n",
    "spec_gen = load_spectrogram_model().cuda()\n",
    "vocoder = load_vocoder_model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocder_model, str_input):\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen.parse(text_to_generate)\n",
    "        spectrogram = spec_gen.generate_spectrogram(tokens=parsed)\n",
    "        audio = vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    if isinstance(spectrogram, torch.Tensor):\n",
    "        spectrogram = spectrogram.to('cpu').numpy()\n",
    "    if len(spectrogram.shape) == 3:\n",
    "        spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input what you want the model to say: hello\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_spectrogram() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fb43e32bc862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input what you want the model to say: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_to_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-bdc419a55584>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(spec_gen_model, vocder_model, str_input)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mspectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_spectrogram_to_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_spectrogram() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "text_to_generate = input(\"Input what you want the model to say: \")\n",
    "spec, audio = infer(spec_gen, vocoder, text_to_generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Audio and Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "ipd.Audio(audio, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "imshow(spec, origin=\"lower\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
